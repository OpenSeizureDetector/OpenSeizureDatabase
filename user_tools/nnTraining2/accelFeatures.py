import numpy as np
from scipy.signal import butter, filtfilt, welch
from scipy.signal.windows import hann
from scipy.stats import skew, kurtosis

"""
    This code was initially generated by Googgle Gemini, which suggested the following text prompt that would regenerate the code
    from the conversation:

        "Provide a complete, production-ready Python script to process a continuous stream of accelerometer data (x, y, z arrays). The script should include functions for:
        1.  High-pass filtering the entire data stream to remove gravity and low-frequency noise.
        2.  Segmenting the filtered data into overlapping epochs of a specified length (`epoch_length`) and step size (`step`), both of which can be floating-point numbers in seconds.
        3.  For each epoch, calculate a comprehensive set of statistical and frequency-domain features for each axis (x, y, z) and for the vector magnitude. The features should include:
            * **Time-Domain:** activity count (RMS), mean, standard deviation, skewness, kurtosis, and zero-crossing rate.
            * **Frequency-Domain:** total power and peak PSD for multiple specified frequency bands, along with mean frequency and frequency-domain entropy.
        4.  The final function should return a list of dictionaries, where each dictionary contains all the calculated features for one epoch.
        5.  Include a separate function to apply Min-Max scaling to the returned list of feature dictionaries, transforming them into a pandas DataFrame ready for a machine learning model.
        6.  Ensure the code is well-commented and includes a main execution block with a clear example of how to use the functions."

"""

def butter_highpass_filter(data, cutoff, fs, order=4):
    """
    Applies a Butterworth high-pass filter to the data.

    Args:
        data (np.ndarray): The input signal to filter.
        cutoff (float): The cutoff frequency of the filter in Hz.
        fs (int): The sampling frequency of the data in Hz.
        order (int): The order of the filter.

    Returns:
        np.ndarray: The filtered data.
    """
    nyq = 0.5 * fs  # Nyquist frequency
    normal_cutoff = cutoff / nyq
    
    b, a = butter(order, normal_cutoff, btype='high', analog=False)
    y = filtfilt(b, a, data)
    return y

def calculate_epoch_features(accel_data, sf, freq_bands):
    """
    Calculates a comprehensive set of features for a single epoch of accelerometer data.

    Args:
        accel_data (dict): A dictionary containing 'x', 'y', and 'z' numpy arrays for the epoch.
        sf (int): The sampling frequency in Hz.
        freq_bands (dict): A dictionary of frequency bands to analyze.
            Example:
                freq_bands = {
                    'seizure_main': (1.0, 4.0),
                    'initial_clonus': (3.0, 5.0),
                    'late_clonus': (1.0, 2.0)
                }

    Returns:
        dict: A dictionary containing all calculated features for the epoch.
    """
    features = {}
    
    # Per-axis and vector magnitude feature calculation
    data_sources = {
        'x': accel_data['x'],
        'y': accel_data['y'],
        'z': accel_data['z'],
        'magnitude': np.sqrt(accel_data['x']**2 + accel_data['y']**2 + accel_data['z']**2)
    }

    for source_name, data in data_sources.items():
        # --- Time-Domain Features ---
        features[f'activity_count_{source_name}'] = np.sqrt(np.mean(data**2))
        features[f'mean_{source_name}'] = np.mean(data)
        features[f'std_{source_name}'] = np.std(data)
        features[f'skewness_{source_name}'] = skew(data)
        features[f'kurtosis_{source_name}'] = kurtosis(data)
        
        # Zero-Crossing Rate (ZCR)
        features[f'zcr_{source_name}'] = np.sum(np.diff(np.sign(data))) / (2 * len(data))
        
        # --- Frequency-Domain Features ---
        nperseg = sf * 2
        nperseg = None
        frequencies, psd = welch(data, fs=sf, nperseg=nperseg, window=hann(len(data)), scaling='density')
        
        total_psd_power = np.sum(psd)
        if total_psd_power > 0:
            # Mean Frequency (Centroid)
            features[f'mean_freq_{source_name}'] = np.sum(frequencies * psd) / total_psd_power
            
            # Frequency-Domain Entropy
            psd_norm = psd / total_psd_power
            features[f'entropy_{source_name}'] = -np.sum(psd_norm * np.log2(psd_norm + 1e-12))
        else:
            features[f'mean_freq_{source_name}'] = 0
            features[f'entropy_{source_name}'] = 0
            
        # Calculate peak power and total power for each specified frequency band
        for band_name, (lower, upper) in freq_bands.items():
            freq_indices = np.where((frequencies >= lower) & (frequencies <= upper))
            psd_in_band = psd[freq_indices]
            
            features[f'total_power_{source_name}_{band_name}'] = np.sum(psd_in_band) if psd_in_band.size > 0 else 0
            features[f'peak_psd_{source_name}_{band_name}'] = np.max(psd_in_band) if psd_in_band.size > 0 else 0
    
    return features


def process_accelerometer_data(accel_x, accel_y, accel_z, sf, epoch_length=10.0, step=5.0, freq_bands={'seizure_main': (1.0, 4.0), 'initial_clonus': (3.0, 5.0), 'late_clonus': (1.0, 2.0)}):
    """
    Processes a continuous stream of accelerometer data into overlapping epochs and calculates features.
    
    Args:
        accel_x (np.ndarray): Full accelerometer data for the x-axis.
        accel_y (np.ndarray): Full accelerometer data for the y-axis.
        accel_z (np.ndarray): Full accelerometer data for the z-axis.
        sf (int): The sampling frequency in Hz.
        epoch_length (float): The length of each epoch in seconds.
        step (float): The step size between epochs in seconds (for overlapping).
        freq_bands (dict): A dictionary of frequency bands to analyze.

    Returns:
        list: A list of dictionaries, where each dictionary contains the calculated features for one epoch.
    """
    if not (len(accel_x) == len(accel_y) == len(accel_z)):
        raise ValueError("All accelerometer data arrays must have the same length.")

    # 1. High-pass filter the entire data stream
    hp_cutoff = 0.5
    accel_x_filtered = butter_highpass_filter(accel_x, hp_cutoff, sf)
    accel_y_filtered = butter_highpass_filter(accel_y, hp_cutoff, sf)
    accel_z_filtered = butter_highpass_filter(accel_z, hp_cutoff, sf)

    # Convert epoch length and step to sample counts
    epoch_samples = int(epoch_length * sf)
    step_samples = int(step * sf)
    
    feature_list = []
    start = 0

    # 2. Segment the data into overlapping epochs
    while start + epoch_samples <= len(accel_x):
        end = start + epoch_samples
        
        epoch_data = {
            'x': accel_x_filtered[start:end],
            'y': accel_y_filtered[start:end],
            'z': accel_z_filtered[start:end]
        }
        
        # Calculate features for the epoch
        features = calculate_epoch_features(epoch_data, sf, freq_bands)
        feature_list.append(features)

        # Move to the next epoch
        start += step_samples
    
    return feature_list

# Example Usage:
if __name__ == '__main__':
    # Generate continuous dummy data for 30 seconds
    sf = 25
    total_seconds = 30
    total_samples = sf * total_seconds
    t = np.linspace(0, total_seconds, total_samples, endpoint=False)
    
    # Data with a slow movement and a rhythmic event in the middle
    slow_movement = np.sin(2 * np.pi * 0.1 * t) * 5
    seizure_motion_x = np.where((t >= 10) & (t < 20), np.cos(2 * np.pi * 2 * t) * 10, 0)
    seizure_motion_y = np.where((t >= 10) & (t < 20), np.sin(2 * np.pi * 2 * t) * 8, 0)
    seizure_motion_z = np.where((t >= 10) & (t < 20), np.sin(2 * np.pi * 2 * t) * 5, 0)
    
    noise = np.random.randn(total_samples) * 0.5
    
    x_data = slow_movement + seizure_motion_x + noise
    y_data = slow_movement + seizure_motion_y + noise
    z_data = slow_movement + seizure_motion_z + noise
    
    try:
        # Process the continuous data stream with a 10-second epoch and a 5-second step
        all_epochs_features = process_accelerometer_data(x_data, y_data, z_data, sf=sf, epoch_length=10, step=5)
        
        # Print features for each epoch
        for i, features in enumerate(all_epochs_features):
            start_time = i * 5
            end_time = start_time + 10
            print(f"\n--- Epoch {i+1} (Time: {start_time}s to {end_time}s) ---")
            for feature_name, value in features.items():
                print(f"{feature_name}: {value:.2f}")

    except ValueError as e:
        print(e)